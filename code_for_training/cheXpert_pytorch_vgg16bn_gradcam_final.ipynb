{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlPuo4eFwB1p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DQrdeKZxQgq"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkJ-Xk-eIk7P"
   },
   "source": [
    "## Assistant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_QVXd0MR8bX"
   },
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accs(pred_labels, true_labels):\n",
    "    \"\"\"\n",
    "    function that computes accuracy for each input/image.\n",
    "    \n",
    "    @params: pred_labels: Tensor, labels that are predicted by fitted model\n",
    "             true_labels: Tensor, true labels that come with inputs\n",
    "    @return: Float, accuracy of each input/image\n",
    "    \"\"\"\n",
    "    \n",
    "    num_classes = pred_labels.shape[1]\n",
    "    result = torch.round(pred_labels).eq(true_labels).sum(axis=1)\n",
    "    return result.detach().cpu().numpy() / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_images_labels(images, labels, names, accuracies, threshold, image_holder, label_holder, name_holder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that selects image(s) that meet \n",
    "    the threshold and puts them into an image_holder.\n",
    "    The corresponding labels are put into a label_holder.\n",
    "    \n",
    "    @params: images: Tensor, a batch of images\n",
    "             labels: Tensor, corresponding true labels of images\n",
    "             accuracies: Tensor, prediction accuracies of each image\n",
    "             threshold: Float, 1.0 or 0.5 to find best and worst predicted images\n",
    "             image_holder: List, list to store best and worst images\n",
    "             label_holder: List, list to store labels of those images\n",
    "    \n",
    "    @return: None\n",
    "    \"\"\"\n",
    "\n",
    "    ind = np.where(accuracies==threshold)[0]\n",
    "\n",
    "    imgs = images[ind]\n",
    "    img_labels = labels[ind]\n",
    "    img_names = names[ind]\n",
    "    \n",
    "    if len(ind) != 0:\n",
    "        for img in imgs:\n",
    "            img = img.detach().cpu().numpy()\n",
    "            image_holder.append(np.expand_dims(img, 0))\n",
    "        for img_label in img_labels:\n",
    "            label_holder.append(img_label)\n",
    "        for img_name in img_names:\n",
    "            name_holder.append(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(model, image):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that creates a heatmap for a specific class of an image.\n",
    "    \n",
    "    @params: model: trained model that uses to make predictions.\n",
    "             image: Tensor, image that is ready to be predicted.\n",
    "    \n",
    "    @return: heatmap: Tensor, heatmap shows locations that the model is\n",
    "                      looking for to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # pull the gradients out of the model\n",
    "    gradients = model.get_activations_gradient()\n",
    "\n",
    "    # pool the gradients across the channels\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "    # get the activations of the last convolutional layer\n",
    "    activations = model.get_activations(image).detach()       # detach() stops tracking gradients\n",
    "\n",
    "    # # weight the channels by corresponding gradients\n",
    "    for i in range(512):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "    # average the channels of the activations\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()    # average over 512 channels; squeeze() to (14, 14)\n",
    "\n",
    "    # relu on top of the heatmap\n",
    "    # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "    heatmap = np.maximum(heatmap.cpu(), 0)        # ReLU; np.maximum() element-wise max\n",
    "\n",
    "    # normalize the heatmap\n",
    "    heatmap /= torch.max(heatmap)\n",
    "\n",
    "    # draw the heatmap\n",
    "#     plt.matshow(heatmap.squeeze())\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that predicts probabilities and classes of an image.\n",
    "    \n",
    "    @params: model: trained model that is ready for predictions\n",
    "             image: Tensor, an image that is to be predicted\n",
    "    \n",
    "    @return: pred: Tensor, predicted probabilities for each class\n",
    "             output_classes: List\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # get the positive prediction(s) of the model\n",
    "    pred = model.forward_train(image)\n",
    "    \n",
    "    # get labels of the image\n",
    "    labels = torch.round(torch.sigmoid(pred)).cpu()\n",
    "\n",
    "    # get output class(es)\n",
    "    output_classes = np.where(labels == 1)[1]\n",
    "    \n",
    "    return pred, output_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_CAM(model, image, labels, image_path, save_dir, file_type):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that performs grad-CAM on an image and saves the superimposed image.\n",
    "    \n",
    "    @params: model: trained model that is ready for predictions\n",
    "             image: Numpy Array, an image that is to be predicted\n",
    "             labels: Tensor, labels corresponding to the image\n",
    "             image_path: Numpy String, directory where the original image is saved\n",
    "             save_dir: String, directory where to save output\n",
    "             file_type: String, output type\n",
    "    \n",
    "    @return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    img = torch.tensor(image).clone().detach().cuda()\n",
    "    pred, output_classes = predict(model, img)\n",
    "    print(pred)\n",
    "    print(output_classes)\n",
    "    print(np.array(classes)[output_classes])\n",
    "\n",
    "    # verify output classes and true classes\n",
    "#     true_labels = np.where(labels.cpu()==1)[0]\n",
    "#     print(true_labels == output_classes)\n",
    "\n",
    "    # backprop\n",
    "    # get the gradient of the support devices wrt the parameters of the model\n",
    "    for j in range(len(output_classes)):\n",
    "\n",
    "        pred, output_classes = predict(model, img)\n",
    "        pred[:, output_classes[j]].backward()\n",
    "\n",
    "        # generate heatmap\n",
    "        heatmap = create_heatmap(model, img)\n",
    "\n",
    "        # load in original image\n",
    "        im = cv2.imread(image_path)\n",
    "\n",
    "        # resize heatmap to match original image\n",
    "        heatmap = cv2.resize(np.float32(heatmap), (im.shape[1], im.shape[0]))\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "        superimposed_img = cv2.addWeighted(im, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "        # save superimposed image (original+heatmap)\n",
    "        patient_name = image_path[29:41]\n",
    "        class_name = '_' + np.array(classes)[output_classes[j]]\n",
    "        savePath = save_dir + patient_name + class_name + file_type\n",
    "        cv2.imwrite(savePath, superimposed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JSCb1rOC6L2y"
   },
   "source": [
    "## 1. Create Dataset\n",
    "We create our dataset from a csv file which includes image names and mask of each image. Each mask includes 14 different types of pathologies. It's possible that each image has more than one pathologies, indicating we use multi-hot encoding in this project.<br><br>\n",
    "_Mask Type:_\n",
    "* Blanks (''): the existence of the pathology is unknown.\n",
    "* Ones (1): the pathology is detected thanks to the image.\n",
    "* Zeros (0): the pathology can't be detected thanks to the image.\n",
    "* Uncertain (-1): the pathology may be detected.\n",
    "\n",
    "_Policies:_\n",
    "* ones: seeing all the uncertain pathologies as positive\n",
    "* zeros: seeing all the uncertain pathologies as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOGZGehZwB1u"
   },
   "outputs": [],
   "source": [
    "# Paths to the files with training, and validation sets.\n",
    "# Each file contains pairs (path to image, output vector)\n",
    "pathFileTrain = './train_50000.csv'\n",
    "pathFileValid = './valid.csv'\n",
    "pathFileTest = './test.csv'\n",
    "\n",
    "# Neural network parameters:\n",
    "nnClassCount = 14                   #dimension of the output\n",
    "\n",
    "# Training settings: batch size\n",
    "trBatchSize = 64\n",
    "# Test settings: batch size\n",
    "tsBatchSize = 32\n",
    "\n",
    "# Parameters related to image transforms: size of the down-scaled image, cropped image\n",
    "imgtransResize = (320, 320)\n",
    "imgtransCrop = 224\n",
    "\n",
    "# class names\n",
    "classes = ['No_Finding', 'Enlarged_Cardiomediastinum', 'Cardiomegaly', 'Lung_Opacity', \n",
    "           'Lung_Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "           'Pleural_Effusion', 'Pleural_Other', 'Fracture', 'Support_Devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NQUBySzwB1z"
   },
   "outputs": [],
   "source": [
    "class CheXpertDataSet(Dataset):\n",
    "    def __init__(self, image_list_file, transform=None, policy=\"ones\"):\n",
    "        \"\"\"\n",
    "        image_list_file: path to the file containing images with corresponding labels.\n",
    "        transform: optional transform to be applied on a sample.\n",
    "        Upolicy: name the policy with regard to the uncertain labels\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        with open(image_list_file, \"r\") as f:\n",
    "            csvReader = csv.reader(f)\n",
    "            next(csvReader, None)\n",
    "            k=0\n",
    "            for line in csvReader:\n",
    "                k+=1\n",
    "                image_name= line[0]\n",
    "                label = line[5:]\n",
    "                \n",
    "                for i in range(14):\n",
    "                    if label[i]:\n",
    "                        a = float(label[i])\n",
    "                        if a == 1:\n",
    "                            label[i] = 1\n",
    "                        elif a == -1:\n",
    "                            if policy == \"ones\":\n",
    "                                label[i] = 1\n",
    "                            elif policy == \"zeroes\":\n",
    "                                label[i] = 0\n",
    "                            else:\n",
    "                                label[i] = 0\n",
    "                        else:\n",
    "                            label[i] = 0\n",
    "                    else:\n",
    "                        label[i] = 0\n",
    "                        \n",
    "                x.append('../' + image_name)\n",
    "                y.append(label)\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n",
    "        \n",
    "        image_name = self.x[index]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        label = self.y[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return (image, torch.FloatTensor(label), (image_name,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2tyA8DFwB11"
   },
   "source": [
    "## 2. Create DataLoaders\n",
    "\n",
    "First we define a transform model to resize all images and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZCDvMJ9wB12"
   },
   "outputs": [],
   "source": [
    "#TRANSFORM DATA\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "transformList = []\n",
    "transformList.append(transforms.Resize(imgtransResize))\n",
    "transformList.append(transforms.RandomResizedCrop(imgtransCrop))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence = transforms.Compose(transformList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwMb3lZJwB13"
   },
   "source": [
    "Then we build train, validation and test data loaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXHjaDpLwB14"
   },
   "outputs": [],
   "source": [
    "#LOAD DATASET\n",
    "train_ds = CheXpertDataSet(pathFileTrain ,transformSequence, policy=\"ones\")\n",
    "test_ds, train_ds = random_split(train_ds, [2000, len(train_ds) - 2000])\n",
    "valid_ds = CheXpertDataSet(pathFileValid, transformSequence)\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaderTrain = DataLoader(dataset=train_ds, batch_size=trBatchSize, shuffle=True)\n",
    "dataloaderValid = DataLoader(dataset=valid_ds, batch_size=trBatchSize, shuffle=False)\n",
    "dataloaderTest = DataLoader(dataset=test_ds, batch_size=trBatchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWgGINQbJR8f"
   },
   "source": [
    "Plot four sample images from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1THLd5V4B32"
   },
   "outputs": [],
   "source": [
    "# load some random training images\n",
    "images, labels = iter(dataloaderTrain).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1577742114159,
     "user": {
      "displayName": "Qin Hu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD__pR-MhUPSehMt0cAKrqH4lYZG8qxYmk_yjCK=s64",
      "userId": "06372700396773093593"
     },
     "user_tz": 300
    },
    "id": "-aWBFqb9RhtT",
    "outputId": "414d2b1b-7f4b-4158-c0e1-2f412784e400"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,9), ncols=4)\n",
    "for ii in range(4):\n",
    "    ax = axes[ii]\n",
    "    imshow(images[ii], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-52eZqvKta-"
   },
   "source": [
    "## 3. Create Architecture and Train Model\n",
    "\n",
    "Create a customized vgg16_bn architecture, find the best learning rate, and train the model using best slice learning rates.\n",
    "\n",
    "* Find the last convolution layer, which is the 42nd layer of the vgg16bn model\n",
    "* Perform the first 42nd features and hook the gradients on the last convolution layer\n",
    "* Perform the remaining features and the classifier\n",
    "* Create two methods to extract the gradients and the activation map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxFocmD5tZRg"
   },
   "outputs": [],
   "source": [
    "# customized model\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # get the pretrained vgg16_bn network\n",
    "        self.vgg = models.vgg16_bn(pretrained=True)\n",
    "        \n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = self.vgg.features[:43]\n",
    "        \n",
    "        # get the avg pool\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        # get the max pool of the features stem\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the classifier of the vgg16_bn\n",
    "        self.classifier = nn.Sequential(nn.Linear(25088, 4096),\n",
    "                          nn.ReLU(True),\n",
    "                          nn.Dropout(0.5),\n",
    "                          nn.Linear(4096, 4096),\n",
    "                          nn.ReLU(True),\n",
    "                          nn.Dropout(0.5),\n",
    "                          nn.Linear(4096, nnClassCount))\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward_train(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining pooling\n",
    "        x = self.max_pool(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        # only register the hook in training\n",
    "        x = self.features_conv(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98xdk_45HiPR"
   },
   "source": [
    "Replace the classifier of the pretrained model to the customized one matching our number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gilB1PijHiPS"
   },
   "source": [
    "Define loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVcUuoBEHiPZ",
    "outputId": "81abab9a-7b49-459b-dacb-4e38d98b4eda"
   },
   "outputs": [],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize the VGG model\n",
    "model = VGG()\n",
    "    \n",
    "# loss function\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-5)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eIfZGN6HiPe"
   },
   "source": [
    "Train the VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yf8hG4PGHiPf",
    "outputId": "bc3ea4ea-41c5-49b3-e8fa-67f08bce4abe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "\n",
    "# metrics to save the best model\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels, _ in dataloaderTrain:\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model.forward_train(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels, _ in dataloaderValid:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward_test(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    \n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Calculate scores\n",
    "                    pred_labels = torch.round(torch.sigmoid(logps))\n",
    "                    accs = compute_accs(pred_labels, labels)\n",
    "                    accuracy += np.mean(accs)\n",
    "            \n",
    "            # save the best model\n",
    "            if accuracy/len(dataloaderValid) > best_acc:\n",
    "                best_model = model\n",
    "                \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(dataloaderValid):.3f}.. \"\n",
    "                  f\"Acc: {accuracy/len(dataloaderValid):.3f}.. \")\n",
    "            \n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "#         else:\n",
    "#             print('.')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(best_model.state_dict(), 'best_model_vgg16bn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict Test Data Using Best Model\n",
    "* Predict test data using the best model saved in the training session.\n",
    "* Extract 10 images that have an accuracy of 100% and 10 images that have an accuracy not greater than 50%.\n",
    "* Perform Grad-CAM on these 20 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test dataset and test dataloader using a smaller batch size to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATASET\n",
    "test_ds = CheXpertDataSet(pathFileTest, transformSequence)\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaderTest = DataLoader(dataset=test_ds, batch_size=tsBatchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num test batches:', len(dataloaderTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create four empty lists `best_images`, `worst_images`, `best_labels` and `worst_labels`. `best_images` stores images that have an accuracy of 100%, while `worst_images` stores those that have an accuracy of <= 50. The other two lists store corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_images = []\n",
    "worst_images = []\n",
    "best_labels = []\n",
    "worst_labels = []\n",
    "best_names = []\n",
    "worst_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict outcomes for each batch of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "for images, labels, image_names in dataloaderTest:\n",
    "    image_names = np.asarray(image_names)[0]\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    pred = best_model.forward_test(images)\n",
    "    \n",
    "    # get labels of the image\n",
    "    pred_labels = torch.round(torch.sigmoid(pred))\n",
    "\n",
    "    # get accuracy of each image\n",
    "    accs_test = compute_accs(pred_labels, labels)\n",
    "    print('acc:', np.mean(accs_test))\n",
    "    \n",
    "    select_images_labels(images, labels, image_names, accs_test, 1.0, best_images, best_labels, best_names)\n",
    "    select_images_labels(images, labels, image_names, accs_test, 0.5, worst_images, worst_labels, worst_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('num best images:', len(best_images))\n",
    "print('num worst images:', len(worst_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of images that are labeled all 0's and get their indices. Get rid of these images from `best_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_lst = []\n",
    "for k in range(len(best_labels)):\n",
    "    if sum(best_labels[k]) == 0:\n",
    "        invalid_lst.append(k)\n",
    "        \n",
    "print(invalid_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ind_lst = set(np.arange(len(best_images)))\n",
    "valid_lst = list(full_ind_lst - set(invalid_lst))\n",
    "print(valid_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_images_new = np.array(best_images)[valid_lst]\n",
    "best_labels_new = np.array(best_labels)[valid_lst]\n",
    "best_names_new = np.array(best_names)[valid_lst]\n",
    "print(best_images_new.shape)\n",
    "print(best_labels_new.shape)\n",
    "print(best_names_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydUxDu3bKqB0"
   },
   "source": [
    "Save the session, in case the model needs to be re-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dill.dump_session('vgg16bn_images_selected.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dill.load_session('vgg16bn_images_selected.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYLjGSlLK_Fs"
   },
   "source": [
    "## 4. Perform Grad-CAM on Each Selected Image from Valid Set\n",
    "* Predict logits for each image regarding to the 14 labels\n",
    "* Apply `torch.sigmoid()` to convert logits to binary labels\n",
    "* Specify output class and call `backward()` to get the gradients in terms of the class\n",
    "* Perform formulas in the paper (https://arxiv.org/pdf/1610.02391.pdf) for Grad-CAM coarse discriminative heatmap\n",
    "* Resize the heatmap to match the original image and combine the two of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKiommqANFpf"
   },
   "source": [
    "Perform grad-CAM on best images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(best_names_new[i][29:41], ':')\n",
    "    grad_CAM(best_model, best_images_new[i], best_labels_new[i], best_names_new[i], save_dir='./grad-cam_results/best/', file_type='.jpg')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform grad-CAM on worst images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform Grad-CAM on all worst images\n",
    "# because some of the predictions are all 0's\n",
    "# where the true labels have seven 1's\n",
    "# these images have no Grad-CAM outputs\n",
    "for i in range(14):\n",
    "    print(worst_names[i][29:41], ':')\n",
    "    grad_CAM(best_model, worst_images[i], worst_labels[i], worst_names[i], save_dir='./grad-cam_results/worst/', file_type='.jpg')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cheXpert_pytorch_vgg16bn_gradcam.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/gaetandi/cheXpert/blob/master/cheXpert_final.ipynb",
     "timestamp": 1568145462194
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
